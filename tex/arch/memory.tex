\section{Memory}

\subsection{Virtual memory}

Modern operating systems for most machines which are not small embedded
platforms do not expose memory directly to processes.  Instead, a hardware
component called a \textit{Memory Management Unit} (MMU) mediates access to
physical memory.  Memory as seen by the processes is called \textit{virtual
memory}.  The MMU translates the \textit{virtual memory addresses} used by the
processes into real, \textit{physical memory addresses} according to the
configuration done by the operating system.  Virtual memory is thus a
software-controlled set of memory addresses.

This mapping is not restricted to simply address translation.  Any time an
access to an unmapped address occurs, a \textit{page fault} is generated (memory
is divided in pages, as will be explained later).  This is analogous to a
hardware interrupt, and the operating system can configure a handler which will
direct the MMU to take one of several possible actions to resolve the address:

\begin{itemize}
    \item
        Immediately assign a physical memory address and service the memory
        request.  This is called a \textit{soft page fault}.
    \item
        Defer the decision to the operating system.  This is called a
        \textit{hard page fault}.
    \item
        Immediately fail the memory access.  This is the source of the
        (in)famous \textit{segmentation fault} or \textit{access violation}
        failure.
\end{itemize}

This level of indirection is only practically possible because it is implemented
in part in hardware.  It is nonetheless immensely useful: the possibility of
deferral of faults to a software process allows the implementation of several
mechanisms which are widely used in modern operating systems and programs.

\subsubsection{Virtual address space}

From the operating system's perspective, one of the most important is the
possibility of setting up distinct mappings for each process, which are also
separate from that of the kernel.  This is the fundamental method of separation
between processes and between user and kernel space.  Because of the
unprivileged nature of user mode (MMU configuration is only allowed when the CPU
is in kernel mode), the operating system can reconfigure mappings whenever it
schedules a new process to guarantee it only has access to its own view of
memory.  Kernel memory is thus protected from user space access, and each
process is restricted to its own address space\footnotemark.

\footnotetext{
    Recently, even this coarse dichotomy has been deemed insufficient, and
    further separation between distinct areas of kernel memory is being
    considered, e.g. \url{https://lwn.net/Articles/886494/}.}

This level of indirection in memory accesses can also be used to radically
change the contents of physical memory in a way that is transparent to the user
processes:

\begin{itemize}
    \item
        Since there is no relation between virtual and physical addresses, a
        large contiguous memory allocation can be serviced by dividing it into
        several smaller ones, potentially reducing overall memory fragmentation.
    \item
        Process data do not even have to reside in memory at all: when system
        memory is scarce --- either due to allocations or caching ---
        less-frequently-used memory regions can be move to slower storage (such
        as a hard drive) to make space and if/when they are requested again, a
        process called \textit{swapping}.

        Doing so takes advantage of \textit{locality of reference}, a common
        memory access pattern in the execution of programs: memory tends to be
        referenced locally repeatedly, either spatially (adjacent regions) or
        temporally (repeating accesses, such as in a loop).  Only the relevant,
        relatively small regions which are presently referenced need to be in
        memory, in what is called the \textit{resident set} of the
        process\footnotemark.
    \footnotetext{
        As displayed, for example, in the \texttt{rss} (resident set size) field
        in \texttt{ps(1)} or the \texttt{RES} column in \texttt{top(1)}.}
    \item
        The operating system and the MMU may also support \textit{memory
        protection}, usually in the form of several permission bits which
        control access to certain memory regions.  Protection bits can also be
        used to make regions read-only, prevent their execution as code,
        completely forbid access, etc.
    \item
        Regions can be shared by multiple processes.  This can happen when
        multiple processes are created for the same program or use the same
        shared library (the text segment can be shared), as a result of the
        \texttt{fork} system call (all memory is initially shared), or via
        explicit requests (using \texttt{shmget(2)}, \texttt{mmap(2)}, etc.).
        These common regions can be set up in either \textit{private} or
        \textit{shared} mode.  Modifications made to private regions are not
        visible to other processes: the region is duplicated by the operating
        system when it is first written.
    \item
        Memory regions do not have to be allocated or initialized immediately:
        they can simply be reserved and serviced when needed.  For example, a
        special segment in a process' address space is the BSS segment (the name
        originates historically from \textit{block started by symbol}), a region
        of memory which is guaranteed to be zero-initialized\footnotemark.  When
        a process is started, this region is not allocated by the operating
        system: it is simply marked as ``zero-initialized'' and materialized on
        first access.
    \footnotetext{
        In C, it corresponds to variables of static storage duration declared
        \texttt{extern} and default- or zero-initialized.}
    \item
        Files can be directly mapped as part of the virtual address space of a
        process.  This can reduce the overhead of making multiple system calls
        to read and write data: the access can be made as if the contents of the
        file were a regular memory region and is handled transparently by the
        MMU.
\end{itemize}

\subsubsection{Pages}

Due to the size of physical memory commonly found in current systems --- on the
order of billions of bytes (GiB) --- it is prohibitive to treat each memory
address individually.  Memory is instead divided into \textit{pages}: contiguous
regions of a predefined size.  A common page size, used in the Linux kernel, is
4096 bytes (4KiB)\footnotemark.  This affects several aspects of the system:
many operations are required to be \textit{page-aligned} --- i.e. are restricted
to begin at page boundaries --- for this reason.  Similarly, MMU configuration
is usually done in terms of pages, in what are called \textit{page tables}.

\footnotetext{
    Linux also has \textit{huge pages}, which are considerably larger (anywhere
    from 2MiB to 1GiB), to mitigate some of the problems described in this
    section.}

Even that, however, is not sufficient.  In a 32-bit system --- where a space of
4GiB is addressable --- with 4KiB pages, $2^{20}$ entries would be necessary
($2^{32}/2^{12} = 2^{20}$).  That overhead would be prohibitive for any
non-trivial amount of per-page information, and hopeless in a 64-bit address
space.  Configuration is done instead in multiple levels: virtual addresses are
split in several parts, called \textit{page directories}, one for each level.
Directories can be sparsely allocated, and a single entry can be used for
several contiguous pages, making it possible for mappings of several processes
to remain in memory\footnotemark.  These directories are usually walked in
hardware by the MMU to resolve virtual addresses.

\footnotetext{
    Although note that \textit{address space layout randomization} (ASLR), a
    security measure present in modern operating systems, conflicts with this
    goal.}

\subsubsection{TLB}

Even though virtual memory accesses are mostly performed by hardware components,
doing a full translation on each access would impose a significant overhead.
For this reason, the CPU itself employs a cache for some of the page table
entries (e.g. those in its L1 cache).  This cache is called the
\textit{translation lookaside buffer} (TLB).  Memory accesses whose address are
found in the cache go directly to main memory (or to one of the memory caches).
A cache miss triggers a page lookup as described above; this is an expensive
process and frequent TLB misses can significantly affect the performance of a
program.  This is one of the reasons for the inefficiency of context switches
and process scheduling: they can result in a partial or complete TLB flush,
since a process cannot reuse the virtual memory mappings of another one.

\subsection{Cache}

\label{subsec:arch:cache}

Memory caches are a consequence of the frequency discrepancy between CPU and
memory units.  Initially, these operated at comparable speeds, but as a
consequence of Moore's law processor speeds increased dramatically, greatly
surpassing the improvements in memory units.  A secondary effect was that CPU
frequencies increased such that the distance between these components became a
limiting factor.  Even imagining a scenario where transmission occurred at the
speed of light and in a vacuum --- an already unrealistic scenario --- the
transmission time for a signal between two components at a $1cm$ distance would
be:

\begin{align*}
    c &\approx 3 \times 10^8m/s \\
    F &= \frac{1cm}{c}
       \approx \frac{10^{-2}m}{3 \times 10^8m/s}
       \approx 3 \times 10^{10}Hz \\
\end{align*}

That is, even in ideal conditions a $1cm$ distance limits communication to the
low gigahertz (GHz) range, which is close to the frequency common in modern
processors.  These two factors combined result in CPUs which are orders of
magnitude (usually two or more) faster than the circuits that connect them to
each other and memory systems.  To avoid these long interruptions in execution
whenever a memory access occurs, modern processors have multiple levels of
\textit{cache}.

Two main characteristics distinguish memory caches from main memory: their type
and their size.  System memory is almost exclusively \textit{dynamic RAM}
(DRAM), making large storage arrays cheap and compact, but slow.  Caches, on the
other hand, operate on smaller sets of data at a time, so have lower storage
requirements.  They are usually built with \textit{static RAM} (SRAM), which are
much more complex, but much faster.  Their reduced size (even with the extra
components compared to DRAM) also allows them to be placed closer to the CPUs,
reducing transmission times.  CPUs typically have multiple levels of cache
(\texttt{L1}, \texttt{L2}, etc.), each larger but further, increasing both
storage capacity and access times.  Higher levels can also be shared between two
or more processors.  Common access times for the L1 and L2 caches are around one
and ten cycles, respectively.

In addition, it is common for two separate types of cache to be used:
\textit{instruction} and \textit{data} caches (sometimes called \textit{icache}
and \textit{dcache}).  Program text and data have distinct access patterns and
often reside in different regions of memory, so this separation can result in
better cache utilization.  The instruction cache can be used to store decoded
instructions, speeding up the execution, since instruction decoding is
relatively slow, and reducing latency, especially in case of a pipeline stall.

Cache systems with multiple layers can be either \textit{inclusive} or
\textit{exclusive}, depending on whether data present in lower (i.e. smaller)
layers are also present in upper layers.  Inclusive caches propagate data across
all layers, which exclusive caches can have data stored only in some layers.
Evictions in inclusive caches are faster, since data can simply be flushed to
the next layer.  Conversely, loading data is faster in exclusive caches, since
only a single layer is affected.

\subsubsection{Lines}

Internally, caches operate on fixed-length blocks called \textit{cache lines}.
The length varies by architecture, but is almost always a power of 2 to
facilitate operations (16 to 256 byte cache lines are common, 64 being the most
common).  Whenever there is an access to a memory location, the address is
analyzed and matched to the entries in the cache.  The first access will not
find the line in the cache (a cache \textit{miss}), resulting in a stall as the
line is transferred from main memory (or upper layers of cache).  Subsequent
accesses will use the contents of the line in the cache (a \textit{hit}), as
long as it is not evicted.

The mapping of memory addresses to positions in the cache is determined by the
cache \textit{associativity}.  The memory address is partitioned into different
pieces depending on the various sizes of the components involved.  On a Linux
machine, these sizes can be determined via \texttt{sysfs}:

\begin{lstlisting}
$ d=/sys/devices/system/cpu/cpu0/cache/index0
$ cat $d/coherency_line_size
64
$ cat $d/size
32K
\end{lstlisting}

In a \textit{fully-associative} cache, each memory address can occupy any
position in the cache.  This allows maximum utilization and hit rate, but it
also means a full search through the cache has to be performed to find a given
line.  These searches are often done by a circuit that tests all values in
parallel, so complexity and power consumption make this arrangement only
practical for very small caches.

In contrast, in a \textit{direct-mapped} cache, each memory address has a single
corresponding position in the cache.  In the example above, the 32KB cache can
hold 512 64-byte cache lines ($32\text{KB} / 64 = 2^{15} / 2^6 = 2^9 = 512$).
For a given memory address, the lower 6 bits ($2^6 = 64$) are an offset into the
cache line and are not used in the cache translation.  The next 9 bits ($2^9 =
512$) indicate the position of the line in the cache, called the cache
\textit{set}.  The remaining bits (17 or 49 for 32- and 64-bit addresses) are
the \textit{tag} used to identify to which memory address a given line in the
cache corresponds.  A direct-mapped cache is analogous to a hash table with no
conflict resolution: if the same ``hash'' position (i.e. set) is already
occupied, it is simply evicted and replaced with the new one.

A \textit{set-associative} is a compromise between the simplicity of a
direct-mapped cache and the benefits of a fully-associative cache.  Each set
contains multiple cache lines: after the set is determined for a given address,
it can be placed in any of the lines in the set.  It is analogous to a hash
table with fixed-size buckets and no external chaining.  A direct-mapped cache
is a 1-way set-associative cache; a fully-associative cache with $n$ sets is an
$n$-way set-associative cache.  The cache in the example above in reality is
8-way associative, which means each set has space for eight lines and thus it
has not 512, but 64 sets ($32\text{KB} / 64 / 8 = 2^{15} / 2^6 / 2^3 = 2^6 =
64$).

\begin{lstlisting}
$ cat $d/ways_of_associativity
8
$ cat $d/number_of_sets
64
\end{lstlisting}

The partitioning of the address follows the same rules as before, only now with
the reduced number of sets: 6 bits for the line offset (unchanged), 6 bits for
the set, and 20/52 bits for the tag.
