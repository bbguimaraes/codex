\section{Integers}

\label{sec:arch:int}

\subsection{Binary numbers and modular arithmetic}

\label{subsec:arch:bin}

Computers must operate on finite values.  This fact limits the maximum values of
numbers they can represent and manipulate.  The \textit{word size}\footnotemark
of a computer architecture determines the natural unit of data of the
instruction set and limits the size of numbers directly accessible.  Early
computers had 8-bit words and several word sizes have existed throughout
history, but common sizes are now 32 and 64 bits.

\footnotetext{
    Used here \textit{stricto sensu}, in contrast to the alternative definition
    of ``word'' as the word size of a particular ancestor of the architecture
    that is now maintained for backwards compatibility (e.g.  ``16-bit word'' in
    x86).}

A binary number with $n$ bits, or \textit{binary digits}, has $2^n$ possible
values.  Table \ref{tbl:arch:integers:bits} shows the common word sizes and the
number of integer values they can represent.  An 8-bit byte can represent two
hundred and fifty six distinct values, while that number is approximately sixty
five thousand, four billion, and eighteen quintillion for 16-, 32-, and 64-bit
numbers, respectively\footnotemark.

\footnotetext{
    Note that the progression of powers in radix 2 whose exponents are multiples
    of 10 is close to those in radix 10 whose exponents are multiples of 3,
    thus:
    \begin{alignat*}{11}
        2^{10} &\approx 10^3    & \quad & \delta\; &= 0.0234375 & \qquad &
        2^{40} &\approx 10^{12} & \quad & \delta\; &= 0.0905053 \\
        2^{20} &\approx 10^6    & \quad & \delta\; &= 0.0463257 & \qquad &
        2^{50} &\approx 10^{15} & \quad & \delta\; &= 0.1118216 & \qquad
        \ldots \\
        2^{30} &\approx 10^9    & \quad & \delta\; &= 0.0686774 & \qquad &
        2^{60} &\approx 10^{18} & \quad & \delta\; &= 0.1326383 \\
    \end{alignat*}
    \vspace{-\baselineskip}
}

\begin{figure}[ht]
    \centering
    \begin{tabular}{ccc}
        bits ($n$) & values ($2^n$) & digits ($\log_{10} 2^n$) \\
        \hline
         8 & 256 & 3 \\
        16 & 65,536 & 5 \\
        32 & 4,294,967,296 & 10 \\
        64 & 18,446,744,073,709,551,616 & 20 \\
    \end{tabular}
    \caption{Number of possible values for an \texttt{n}-bit integer}
    \label{tbl:arch:integers:bits}
\end{figure}

This finiteness means operations eventually exceed the minimum and/or maximum
values --- these events are called \textit{underflow} and \textit{overflow},
respectively.  \textit{Overflow exceptions}\footnotemark are one possible
mechanism to detect and handle them: a signal is generated by the hardware as a
result of the execution of the offending instruction.  This is, however, an
expensive process.

\footnotetext{
    Meaning a \emph{hardware} exception/fault in this case, not the
    similar software concept with the same name.}

More importantly, some of the most fundamental mathematical properties of
operations are not retained: \texttt{(x + x) - x} is no longer equal to
\texttt{x + (x - x)}.  The sum in the former can overflow, while the latter will
always equal \texttt{x}; i.e. associativity is not preserved\footnotemark.

\footnotetext{
    Far from a theoretical point, a very similar computation was the source of a
    problem in one of the most famous algorithms (and algorithm books), the
    binary search in Jon Bentley's \textit{Programming Pearls} (v. section
    \secrefpar{sec:algo:bsearch}).}

Another option in the case of integers is to perform \textit{modular
arithmetic}: operations are executed \emph{modulo} the total number of
representable values.  For a binary number with $n$ bits, operations are
performed \emph{modulo} $2^n$.  Modular arithmetic preserves all of the ordinary
mathematical properties of operations:

\begin{alignat*}{5}
    x + y \quad &\equiv_{\bmod n} &\quad& y + x
    && \qquad \text{additive commutativity}
    \\
    (x + y) + z \quad &\equiv_{\bmod n} &\quad& x + (y + z)
    && \qquad \text{additive associativity}
    \\
    x + 0 \quad &\equiv_{\bmod n} &\quad& x
    && \qquad \text{additive unit}
    \\\\
    x + (-x) \quad &\equiv_{\bmod n} &\quad& 0
    && \qquad \text{additive inverse}
    \\
    -(-x) \quad &\equiv_{\bmod n} &\quad& x
    && \qquad \text{cancellation}
    \\\\
    x \times y \quad &\equiv_{\bmod n} &\quad& y \times x
    && \qquad \text{multiplicative commutativity}
    \\
    (x \times y) \times z \quad &\equiv_{\bmod n} &\quad& x \times (y \times z)
    && \qquad \text{multiplicative associativity}
    \\
    x \times 1 \quad &\equiv_{\bmod n} &\quad& x
    && \qquad \text{multiplicative unit}
    \\\\
    x \times (y + z) \quad &\equiv_{\bmod n} &\quad& x \times y + x \times z
    && \qquad \text{distributivity}
    \\
    x \times 0 \quad &\equiv_{\bmod n} &\quad& 0
    && \qquad \text{annihilation}
    \\
\end{alignat*}

\subsubsection{Addition}

The addition of bounded integers can be defined in terms of its inputs --- the
\textit{augend} and the \textit{addend} --- and its outputs --- the \textit{sum}
and the \textit{carry}.  In the case of one-digit binary numbers with modular
arithmetic, the sum is equivalent to an \textit{exclusive or} operation
(\textit{xor}, also denoted as $\oplus$ for that reason).  Similarly, the carry
is equivalent to an \textit{and} operation (also denoted as $\cdot$).  The
resulting equations can be represented in a truth table and encoded in a logic
circuit (figure \ref{fig:arch:half_adder}).

\begin{figure}[ht]
    \centering
    \begin{subfigure}[h]{0.25\textwidth}
        \begin{tabular}{c}
            \begin{lstlisting}[style=c]
s = au ^ ad;
c = au & ad;
            \end{lstlisting}
        \end{tabular}
    \end{subfigure}
    \begin{subfigure}[h]{0.3\textwidth}
        \begin{tabular}{cc|cc}
            \multicolumn{2}{c|}{Input} &
            \multicolumn{2}{c}{Output} \\
            AU & AD & S & C \\
            \hline
            0 & 0 & 0 & 0 \\
            0 & 1 & 1 & 0 \\
            1 & 0 & 1 & 0 \\
            1 & 1 & 0 & 1 \\
        \end{tabular}
    \end{subfigure}
    \begin{subfigure}[h]{0.4\textwidth}
        \input{arch/integers/half_adder.tex}
    \end{subfigure}
    \caption{One-digit binary half adder}
    \label{fig:arch:half_adder}
\end{figure}

This circuit, a \textit{half adder}, can be combined with an external carry
input to form a \textit{full adder} (figure \ref{fig:arch:adder}).  Multiple
full adders can then be connected in sequence to produce an adder for an
arbitrary number of digits, a \textit{ripple-carry adder}.

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}[h]{0.35\textwidth}
        \begin{tabular}{ccc|cc}
            \multicolumn{3}{c|}{Input} &
            \multicolumn{2}{c}{Output} \\
            AU & AD & C$_{in}$ & S & C \\
            \hline
            0 & 0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 1 & 0 \\
            1 & 0 & 0 & 1 & 0 \\
            1 & 1 & 0 & 0 & 1 \\
            \hline
            0 & 0 & 1 & 1 & 0 \\
            0 & 1 & 1 & 0 & 1 \\
            1 & 0 & 1 & 0 & 1 \\
            1 & 1 & 1 & 1 & 1 \\
        \end{tabular}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.5\textwidth}
        \input{arch/integers/adder.tex}
    \end{subfigure}
    \caption{One-digit binary adder}
    \label{fig:arch:adder}
\end{figure}

It is important to note that the result of the addition of two binary numbers
with $n$ bits requires only a single extra bit to be represented.  This is
easily demonstrated since the maximum resulting value is the sum of the maximum
value ($2^n - 1$) with itself:

\begin{align*}
    s        &=   max + max \\
             &=   2^n - 1 + 2^n - 1 \\
             &=   2 \times 2^n - 2 \\
             &=   2^{n + 1} - 2 \\\\
    \log_2 s &\le n + 1 \\
\end{align*}

In an \textit{arithmetic logic unit} (ALU), this means the same type of register
can be used to store the result along with the final carry output of the final
adder in the multi-digit addition circuit.  This carry output is connected to
the \textit{carry flag}, which can be used in instructions following the
addition, including other additions, where it serves as the input carry to the
first adder.  One example of its usage is to implement addition of numbers
larger than the maximum supported integer type: a pair of arrays of numbers can
be summed in sequence, with each iteration adding the carry flag to the next
(more significant) group of digits.

Since integer operations are modulo $2^n$, where $n$ is the word size and the
size of the ALU registers, simply truncating the result to the register size ---
i.e. ignoring the carry --- effects modular arithmetic.  Listing
\ref{lst:arch:add_mod} shows examples of addition with 4-bit integers.

\begin{figure}[ht]
    \begin{lstlisting}[
        caption=Addition with modular arithmetic,
        label=lst:arch:add_mod,
        xleftmargin=0.2\textwidth,
    ]
 0b0100 = 4    0b0111 = 7      0b1100 = 12
+0b0011 = 3   +0b0001 = 1     +0b0111 =  7
-------       -------         -------
      1            10 carry         1
     1            10  carry        1
    1            10   carry      10   carry
-------       -------           10    carry
 0b0111 = 7    0b1000 = 8     -------
                              0b10011 = 19
                              0b1     = 19 / 16
                                      = 19 >> 4
                                      = 1 (carry flag)
                              0b 0011 = 19 % 16
                                      = 19 & 0xf
                                      = 3
    \end{lstlisting}
\end{figure}

\begin{aside}
    The simple concatenation of instances of the adder in figure
    \ref{fig:arch:adder} is not the most efficient configuration for multi-digit
    addition.  This is because the computation of the carry bit for each
    successive digit must wait for the computation of the previous, serializing
    the entire process.

    Alternative designs for the half/full adders allow faster implementations
    than a naive concatenation of adders.  This is achieved by forwarding the
    carry bit to subsequent digits as soon as possible based on the input.
    These designs are called \textit{carry-lookahead adders} for this reason.
    Their basic mode of operation is to group a number of pairs of input digits
    (e.g. 4) under a \textit{lookahead unit}.  The result of the addition
    (\textit{S} in the figure) of each pair is calculated and propagated to the
    unit.  Once it receives its carry input, the unit can immediately calculate
    the carry for the entire group and send it to the next unit, so that it does
    not have to wait for the carry to go through each digit in the group to be
    propagated.
\end{aside}

\subsubsection{Subtraction}

\label{subsubsec:arch:sub}

Subtraction is in many ways analogous to addition: a \textit{minuend},
\textit{subtrahend}, and a \textit{borrow} are combined to produce the
\textit{difference} and the subsequent borrow for each bit, and the last output
borrow affects the carry flag (also called the \textit{borrow flag}.  In fact,
because of modular arithmetic, the computation of the difference for one-digit
binary numbers is exactly the same as that of the sum (figure
\ref{fig:arch:half_subtractor}).  Computing the borrow also uses an \textit{and}
gate but requires an extra \textit{not} gate.  Figure \ref{fig:arch:subtractor}
shows the full subtractor\footnotemark.

\footnotetext{
    Some architectures take advantage of the arithmetic properties described in
    section \secref{subsec:arch:ones_twos_comp} to implement addition and
    subtraction using the same circuit with just a few extra switches to
    pre-process the input values.}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[h]{0.3\textwidth}
        \begin{tabular}{cc|cc}
            \multicolumn{2}{c|}{Input} &
            \multicolumn{2}{c}{Output} \\
            M & S & D & B \\
            \hline
            0 & 0 & 0 & 0 \\
            0 & 1 & 1 & 1 \\
            1 & 0 & 1 & 0 \\
            1 & 1 & 0 & 0 \\
        \end{tabular}
    \end{subfigure}
    \begin{subfigure}[h]{0.3\textwidth}
        \input{arch/integers/half_subtractor.tex}
    \end{subfigure}
    \caption{One-digit binary half subtractor}
    \label{fig:arch:half_subtractor}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[h]{0.3\textwidth}
        \begin{tabular}{ccc|cc}
            \multicolumn{3}{c|}{Input} &
            \multicolumn{2}{c}{Output} \\
            M & S & B$_{in}$ & D & B \\
            \hline
            0 & 0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 1 & 1 \\
            1 & 0 & 0 & 1 & 0 \\
            1 & 1 & 0 & 0 & 0 \\
            \hline
            0 & 0 & 1 & 1 & 1 \\
            0 & 1 & 1 & 0 & 1 \\
            1 & 0 & 1 & 0 & 0 \\
            1 & 1 & 1 & 1 & 1 \\
        \end{tabular}
    \end{subfigure}
    \begin{subfigure}[h]{0.65\textwidth}
        \input{arch/integers/subtractor.tex}
    \end{subfigure}
    \caption{One-digit binary subtractor}
    \label{fig:arch:subtractor}
\end{figure}

Listing \ref{lst:arch:sub_mod} shows examples of subtraction with 4-bit
integers.  As is the case with addition with carry, subtraction with borrow of
binary numbers has the peculiarity that borrowing can also be seen as a bit
flip, as \texttt{1} is the maximum value a digit can have.  Thus, when 1 is
subtracted from a binary number, all lower \texttt{0}s are turned into
\texttt{1}s and the lower \texttt{1} is turned into \texttt{0}.  In other words,
all bits up to and including the lowest \texttt{1} are flipped.  If there is no
bit set --- i.e.  the minuend is zero --- all bits are turned into
\texttt{1}s\footnotemark.  Digits above the lower \texttt{1} are unchanged.

\begin{figure}[ht]
    \begin{lstlisting}[
        caption=Subtraction with modular arithmetic,
        label=lst:arch:sub_mod,
        xleftmargin=0.2\textwidth,
    ]
 0b0111 = 7    0b1100 = 12      0b0010 = 2
-0b0011 = 3   +0b0001 =  1     -0b0100 = 4
-------       -------          -------
      0             1 borrow         0
     0             1                1
    1             0                1   borrow
-------          1                1
 0b0100 = 4   -------          -------
               0b1011 = 11     0b11110 = 30
                               0b1     = 30 / 16
                                       = 30 >> 4
                                       = 1 (borrow flag)
                               0b 1110 = 30 % 16
                                       = 30 & 0xf
                                       = 14
    \end{lstlisting}
\end{figure}

\footnotetext{
    The final value of course depends on the number representation of a
    particular system.  In abstract terms, the \texttt{1}s extend to infinity.
    In a real and finite ALU, all bits in the output register are set to
    \texttt{1}, effecting modular arithmetic.  The carry flag is also set,
    allowing the CPU to handle this underflow case in a variety of ways.}

\begin{figure}[ht]
    \begin{lstlisting}[
        caption={Binary multiplication},
        label=lst:arch:uint_mul,
        xleftmargin=0.2\textwidth,
    ]
    0b0110 =  6          0b0110 =  6 =   6 << 0
   *0b0011 =  3         &0b1111 = 15 = ~(3 >> 0) + 1
----------              -------
    0b0110 =  6 (p0)     0b0110 =  6 (p0)
   0b0110  = 12 (p1)
  0b0000   =  0 (p2)     0b1100 = 12 =   6 << 1
+0b0000    =  0 (p3)    &0b1111 = 15 = ~(3 >> 1) + 1
----------              -------
0b00010010 = 18          0b1100 = 12 (p1)

                        0b11000 = 24 =   6 << 2
                       &0b00000 =  0 = ~(3 >> 2) + 1
                        -------
                        0b00000 =  0 (p2)

                       0b110000 = 48 =   6 << 3
                      &0b000000 =  0 = ~(3 >> 3) + 1
                       --------
                       0b000000 =  0 (p3)
    \end{lstlisting}
\end{figure}

\subsubsection{Multiplication}

The computation of the product of two binary numbers can be separated in several
stages (listing \ref{lst:arch:uint_mul}).  For a binary number of $n$ digits,
the \textit{multiplicand} is first multiplied by each digit of the
\textit{multiplier}, generating $n$ \textit{partial products}.  As with the
carry bit in addition, this is a simple \textit{and} operation, and the result
is all zeroes or the multiplicand unchanged, depending on whether the multiplier
bit is \texttt{0} or \texttt{1}.

The partial products are then multiplied according to the degree of the
multiplier digit that generated them.  Just as with any base, this is a shift
operation for binary numbers.  This also means multiplication, at least from
this point on, has to be done using integers of twice the width of its inputs.
In hardware or software this usually means the destination is either a single
element of a larger type or two elements of the same type as the input (e.g. an
area of a register with twice the width or two separate registers of the same
width).

The final product is the sum of all $n$ partial products.  This is analogous to
the long multiplication method for decimal numbers, but simpler since all
operations on binary numbers can be reduced to logical or shift operations.
Simple architectures reuse the shifter and adder to accumulate partial products
one-by-one.  Faster multipliers have extra circuitry to compute partial products
in parallel and sum them together.  Figure \ref{fig:arch:mul} shows a simple
circuit that multiplies two 2-bit numbers.

\begin{figure}[ht]
    \centering
    \hspace{2em}
    \begin{subfigure}[h]{0.65\textwidth}
        \input{arch/integers/mul.tex}
    \end{subfigure}
    \caption{Binary multiplication}
    \label{fig:arch:mul}
\end{figure}

\subsection{One's and two's complement}

\label{subsec:arch:ones_twos_comp}

All previous examples described natural --- i.e. zero or positive --- numbers.
Negative numbers are defined in relation to them as \textit{additive inverses}:
$-x$ is the number $y$ such that $x + y = 0$:

\begin{align*}
    \forall x \in \mathbb{N}, \quad y = -x \iff x + y = 0 \\
\end{align*}

We have observed this property previously in modular arithmetic: a number $y$ is
the additive inverse of $x$ if $x + y$ is divisible by the modulus (in other
words, if their sum is congruent to zero)\footnotemark.

\footnotetext{
    Note that, by this definition, each number has not one but infinite additive
    inverses: one which is smaller than the modulus and every other number
    congruent to it.  In the example, 15, 31, 47, … are all additive inverses of
    1, since they are all congruent to zero modulus 16 when added to it.}

\begin{align*}
    -1 \equiv_{\bmod{16}} 15
    \quad \because \quad
    \begin{cases}
                 - 1 + 16 = 15 \\
        \phantom{-}1 + 15 = 16 \equiv_{\bmod{16}} 0 \\
    \end{cases} \\
\end{align*}

\begin{figure}[ht]
    \centering
    \begin{tabular}{ccccc|lc}
        \dots & -16 &  0 & 16 & \ldots & $\equiv_{\bmod{16}}  0$ & $= 0000_2$ \\
        \dots & -15 &  1 & 17 & \ldots & $\equiv_{\bmod{16}}  1$ & $= 0001_2$ \\
        \dots & -14 &  2 & 18 & \ldots & $\equiv_{\bmod{16}}  2$ & $= 0010_2$ \\
        \dots & -13 &  3 & 19 & \ldots & $\equiv_{\bmod{16}}  3$ & $= 0011_2$ \\
        \dots & -12 &  4 & 20 & \ldots & $\equiv_{\bmod{16}}  4$ & $= 0100_2$ \\
        \dots & -11 &  5 & 21 & \ldots & $\equiv_{\bmod{16}}  5$ & $= 0101_2$ \\
        \dots & -10 &  6 & 22 & \ldots & $\equiv_{\bmod{16}}  6$ & $= 0110_2$ \\
        \dots &  -9 &  7 & 23 & \ldots & $\equiv_{\bmod{16}}  7$ & $= 0111_2$ \\
        \dots &  -8 &  8 & 24 & \ldots & $\equiv_{\bmod{16}}  8$ & $= 1000_2$ \\
        \dots &  -7 &  9 & 25 & \ldots & $\equiv_{\bmod{16}}  9$ & $= 1001_2$ \\
        \dots &  -6 & 10 & 26 & \ldots & $\equiv_{\bmod{16}} 10$ & $= 1010_2$ \\
        \dots &  -5 & 11 & 27 & \ldots & $\equiv_{\bmod{16}} 11$ & $= 1011_2$ \\
        \dots &  -4 & 12 & 28 & \ldots & $\equiv_{\bmod{16}} 12$ & $= 1100_2$ \\
        \dots &  -3 & 13 & 29 & \ldots & $\equiv_{\bmod{16}} 13$ & $= 1101_2$ \\
        \dots &  -2 & 14 & 30 & \ldots & $\equiv_{\bmod{16}} 14$ & $= 1110_2$ \\
        \dots &  -1 & 15 & 31 & \ldots & $\equiv_{\bmod{16}} 15$ & $= 1111_2$ \\
        \\
    \end{tabular}
    \caption{Modulo space for a 4-bit integer}
    \label{fig:arch:modulo_space}
\end{figure}

Crucially, this extension of modular arithmetic to include negative numbers
still preserves all the properties listed in the beginning of this section
(associativity, commutativity, etc.).  Figure \ref{fig:arch:modulo_space} shows
the same principle applied to every possible value of a 4-bit integer.  This
arrangement naturally leads to partitioning the space of possible values in the
middle to represent negative numbers, obtaining an equal number of negative and
non-negative integers:

\begin{itemize}
    \item
        $[0000_2, 0111_2]$ are positive numbers with the most significant bit
        equal to zero, i.e. $[0, 7]$.
    \item
        $[1000_2, 1111_2]$ are negative numbers with the most significant bit
        equal to one, i.e. $[-8, -1]$.
\end{itemize}

The magnitude of the minimum value is one more than that of the maximum value,
since the value zero takes the place of one of the positive values.  The minimum
value for an \textit{n}-bit number is $-2^{n - 1}$, while the maximum value is
$2^{n - 1} - 1$.  A number added to its negative value results in zero,
preserving the additive inverse property:

\begin{align*}
    7 + (-7) & = 0111_2 + 1001_2 \\
             & = 10000_2 \\
             & = 16 \\
             & \equiv_{\bmod{16}} 0 \\
\end{align*}

\subsubsection{Diminished radix complement}

The complement operation, also called the \textit{diminished radix complement},
is the process of obtaining the complement of a digit with a given radix $r$ by
subtracting it from $r - 1$.  It has no standard mathematical notation, but is
commonly spelled $\sim$ in programming languages such as C (a unary,
right-associative operator), so that is the symbol used here.

For decimal numbers, $r = 10$ and the \textit{nine's complement} of a digit $x$
is $9 - x$.

\begin{align*}
    9 - 7 = 2 \qquad \qquad 9 - 2 = 7 \\
\end{align*}

For binary numbers, $r = 2$ and the \textit{one's complement} of a digit $x$ is
$1 - x$.  This is the same as inverting the value of the bit.

\begin{align*}
    1 - 0 = 1 \qquad \qquad 1 - 1 = 0 \\
\end{align*}

In the same manner, for an $n$-digit number of radix $r$ the diminished radix
complement can be obtained by subtracting it from $r^n - 1$.

\begin{align*}
    \sim7 &= \;\sim0111_2           & \sim8 &= \;\sim1000_2 \\
          &= (r^n - 1) - 0111_2     &       &= (r^n - 1) - 1000_2 \\
          &= (2^4 - 1) - 0111_2     &       &= (2^4 - 1) - 1000_2 \\
          &= (10000_2 - 1) - 0111_2 &       &= (10000_2 - 1) - 1000_2 \\
          &= 1111_2 - 0111_2        &       &= 1111_2 - 1000_2 \\
          &= 1000_2 = 8             &       &= 0111_2 \\
\end{align*}

A negative number is equal to its diminished radix complement plus one:

\begin{align*}
    7 & = 0111_2 \\
      & = -(\sim0111_2 + 1) \\
      & = -(1000_2 + 1) \\
      & = -(1001_2) \\
      & = -(-7) \\
\end{align*}

\subsubsection{Radix complement}

The process of obtaining the negative number described previously is called
the \textit{radix complement}, and consists of subtracting the $n$-digit number
in radix $r$ from $r^n$.  It is equivalent to the diminished radix complement
plus one.  In binary, $r = 2$ and the \textit{two's complement} of a number $x$
of $n$ digits is $2^n - x$.  Because of this, the representation of negative
numbers described in this section is also called \textit{two's complement}.

One peculiarity of this representation is that there are \emph{two} numbers
whose negation equals itself.  One of them is zero, which is expected from
regular arithmetic ($2^n - 0 = 2^n \equiv_{\bmod{2^n}} 0$), but also the minimum
(negative) integer value.  This only happens for this value since its negation
is exactly half of $r^n$.

\begin{align*}
    -(-2^{n - 1}) & = 2^{n - 1} \\
                  & \equiv_{\bmod{2^n}} 2^{n - 1} - 2^n \\
                  & \equiv_{\bmod{2^n}} -2^{n - 1} \\
\end{align*}

\subsubsection{Bitwise right shift}

Because the sign bit has a special meaning in these representations, there are
two possible ways to implement the bitwise shift right operation for integer
values.  Shifting a negative number left has the same effect as doubling it
(including shifting the minimum value, which turns into zero, as $2 \times
-2^{n-1} = -2^n \equiv_{\bmod{2^n}} 0$).  However, shifting to the right also
moves the sign bit, turning a negative number into a large positive one.  This
usually does not make sense mathematically, so some architectures provide an
\textit{arithmetic shift} operation, which is similar to the regular (also
called \textit{logic}) shift but propagates the sign bit instead of filling the
left side with zeroes: \texttt{x >{}> n} sets the $n + 1$ most significant bits
to the value of the sign bit.

\begin{alignat*}{3}
                    0011_2 &= 3 \qquad&\qquad                 1101_2 &= \,&-3 \\
    0011_2 <<   1 = 0110_2 &= 6 \qquad&\qquad 1101_2 <<   1 = 1010_2 &= \,&-6 \\
    0011_2 >>_l 1 = 0001_2 &= 1 \qquad&\qquad 1101_2 >>_l 1 = 0110_2 &= \,& 6 \\
    0011_2 >>_a 1 = 0001_2 &= 1 \qquad&\qquad 1101_2 >>_a 1 = 1110_2 &= \,&-1 \\
\end{alignat*}

\subsubsection{Conversions}

Yet another possible way to interpret an $n$-digit binary number $x$ in two's
complement is to apply the regular $n$-digit polynomial summation equation to
all bits, except the sign bit has its value negated:

\begin{align*}
    x = -b_{n-1}2^{n-1} + \sum_{i=0}^{n-2}b_i2^i \\
\end{align*}

A shortcut when manually calculating the two's complement of a number is to copy
all least significant bits up to and including the rightmost \texttt{1}, then
invert all digits to the left.  This is because, similarly to the reverse
process of subtracting one described in section \ref{subsubsec:arch:sub}, the
one's complement will turn \texttt{0}s into \texttt{1}s, and adding \texttt{1}
to form the two's complement will turn all of them back to \texttt{0} and carry
until the first \texttt{0}, which will be turned back into a \texttt{1}.

\subsubsection{ALU}

The modular nature and the arithmetic properties of integers as implemented by a
computer are preserved in two's complement representation.  The same circuits
can for the most part be used for addition/subtraction/multiplication/division
of signed and unsigned numbers, with only minor modifications to the
descriptions in the previous sections.

\begin{description}
    \item[Subtraction]
        of two numbers $x - y$ in modular arithmetic is the same as the addition
        $x + (m - y)$, where $m$ is the modulus.  It can also be written as $x +
        \sim{}y + 1$, where $\sim{}y$ is the one's complement of $y$, since $(m
        - 1) - y$ is also the complement operation.  Given these equalities, a
        circuit can be built with a switch that will complement the second
        argument and set the carry flag, allowing it to perform additions and
        subtractions while sharing most of the logic.
    \item[Multiplication]
        requires just a few changes to work with integers in two's complement.
        Listing \ref{lst:arch:mul} expands listing \ref{lst:arch:uint_mul} to
        show cases where the multiplicand and/or the multiplier are negative.
        For the former case, the partial products are unchanged, but products
        must be sign-extended to the full width of the resulting type when they
        are added.  For the latter, the partial product resulting from the most
        significant bit of the multiplier is negated, in accordance with the
        digit sum formula for two's complement integers presented earlier.
\end{description}

\subsubsection{Overflow flag}

Because the range of values where the most-significant bit is set now represents
negative numbers, the carry-out signal no longer represents overflow.  For
addition of positive numbers, overflow in this case happens whenever a $1$ is
carried to the most significant bit (i.e. one bit "earlier" than the unsigned
counterpart).  Similarly, underflow happens in the subtraction of negative
numbers whenever a borrow is done from that same most-significant bit.  For this
reason, ALUs typically have two similar but separate flags:

\begin{itemize}
    \item
        The \textit{carry flag} functions exactly as in unsigned operations.
        Because of the reduced range of positive integers, it does not
        distinguish overflow cases in signed operations.
    \item
        The \textit{overflow flag} is used to indicate a \emph{signed} overflow.
\end{itemize}

The value of the overflow flag is dependent not only on the output, but on the
two inputs.  It is set when a negative number (according to the two's complement
representation) results from the addition of two positive numbers or when a
positive number results from the addition of two negative numbers.  Figure
\ref{fig:arch:overflow_flag} shows the truth table and possible circuit
implementation based on the most significant bits of the inputs and the
output\footnotemark.

\footnotetext{
    Note that this isolated circuit is unlikely to be the actual implementation
    in a real ALU.  Parts of the addition/subtraction circuit may be reused; in
    particular, the internal carry bit is commonly used instead of the result.
}

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}[h]{0.3\textwidth}
        \begin{tabular}{ccc|c}
            \multicolumn{3}{c}{MSB} & \\
            X & Y & R & O \\
            \hline
            0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 1 \\
            0 & 1 & 1 & 0 \\
            1 & 0 & 0 & 0 \\
            1 & 1 & 0 & 1 \\
            1 & 0 & 1 & 0 \\
            1 & 1 & 1 & 0 \\
        \end{tabular}
    \end{subfigure}
    \begin{subfigure}[h]{0.5\textwidth}
        \input{arch/integers/overflow_flag.tex}
    \end{subfigure}
    \hfill
    \caption{Overflow flag}
    \label{fig:arch:overflow_flag}
\end{figure}

\begin{figure}[p]
    \begin{lstlisting}[
        caption={Signed binary multiplication},
        label=lst:arch:mul,
        xleftmargin=0.175\textwidth,
    ]
     0b0100 =   4             0b0100 =   4 =    4 << 0
    *0b1101 =  -3            &0b1111 =   1 = ~(-3 >> 0) + 1
 ----------                  -------
     0b0100 =   4 (p0)        0b0100 =   4 (p0)
    0b0000  =   0 (p1)
 + 0b0100   =  16 (p2)        0b1000 =   8 =    4 << 1
 -0b0100    =  32 (p3)       &0b0000 =   0 = ~(-3 >> 1) + 1
 ----------                  -------
    0b10100 =  20             0b0000 =   0 (p1)
 -0b0100000 =  32
 ----------                 0b100000 =  16 =    4 << 2
 0b11110100 = -12          &0b111111 =   1 = ~(-3 >> 2) + 1
                           ---------
                            0b100000 =  16 (p2)

                          0b10000000 =  32 =    4 << 3
                         &0b11111111 =   1 = ~(-3 >> 3) + 1
                         -----------
                          0b10000000 =  32 (p3)



     0b1100 =  -4             0b1100 =  -4 =  -4 << 0
    *0b1101 =  -3            &0b1111 =  15 = ~(-3 >> 0) + 1
 ----------                  -------
 0b11111100 =  -4             0b1100 =  -4 (p0)
    0b0000  =   0
+0b111100   = -16            0b11000 =  -8 =   -4 << 1
-0b11100    = -32           &0b00000 =   0 = ~(-3 >> 1) + 1
 ----------                 --------
 0b11101100 = -20            0b00000 =   0 (p1)
-0b11100000 = -32
 ----------                 0b110000 = -16 =  -4 << 2
 0b00001100 =  12          &0b111111 =  31 = ~(-3 >> 2) + 1
                           ---------
                            0b110000 = -16 (p2)

                           0b1100000 = -32 =  -4 << 3
                          &0b1111111 =  63 = ~(-3 >> 3) + 1
                          ----------
                           0b1100000 = -32 (p3)
    \end{lstlisting}
\end{figure}
